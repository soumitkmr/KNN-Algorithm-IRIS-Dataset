{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Supervised Learning - k-Nearest Neighbor (kNN) Algorithm \n",
    "\n",
    "k-Nearest Neighbor (kNN) algorithm uses a simple idea: \"you are what your neighbors are\". In the first part of the assignment, we will cover some background needed to understand the kNN algorithm. In the second part, you will be asked to apply your knowledge on another data set. \n",
    "\n",
    "## Part A: kNN Tutorial with Questions (50% of grade)\n",
    "\n",
    "Let us start by importing the needed libraries. We will continue using the sklearn library, which implements many of the most popular data science algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the Iris data set using a sklearn function `load_iris`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html, `iris` is an object with features `data` (a 150x4 matrix, where $i$-th row are 4 features of the $i$-th flower), `feature_names` (the names of the 4 features), `target` (a vector of length 150, where $i$-th number is the type of the $i$-th flower -- in machine learning people often say \"label\" instead of \"target\"), `target_names` (these are strings explaining what each of the 3 types of flowers are), and `DESCR` (giving some information about the Iris data set). Let us list them all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print (iris.DESCR)\n",
    "print (iris.data)\n",
    "print (iris.feature_names)\n",
    "print (iris.target)\n",
    "print (iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that the features of the second flower are `[4.9, 3.0, 1.4, 0.2]`, which means its `sepal_length` is 4.9 cm, `sepal_width` is 3.0 cm, `petal_length` is 1.4 cm, and `petal_width` is 0.2 cm. We will write it matematically as $x_2 = [x_{21}, x_{22}, x_{23}, x_{24}] = [4.9, 3.0, 1.4, 0.2]$. We see that its `target` is 0, which means the type of this iris is `setosa`. We will write it matematically as $y_2 = 0$. All this information was obtained by real botanists who studied iris flowers trying to understand the physical measurements that discriminate between the 3 different types of those flowers.\n",
    "\n",
    "In machine learning, people like to denote this data set as $D_{Iris} = \\{(x_i, y_i), i = 1, 2 ... 150\\}$, meaning that data set $D_{Iris}$ is a set of 150 labeled examples $(x_i, y_i)$. An alternative is to write $D_{Iris} = \\{X_{Iris}, Y_{Iris}\\}$.\n",
    "\n",
    "### Supervised Learning\n",
    "Supervised learning is a game with the following objective. You are given the iris data set $D_{Iris}$ where you know 4 features and target values for 150 irises and your objective is to come up with a computer program that predicts a type of any iris flower given the values of its 4 attributes. Written in pseudocode, this is what you have to do:\n",
    "\n",
    "`predictor = create(algorithm_type, D)\n",
    "y_new = predictor(x_new)`\n",
    "\n",
    "In the first line, you are running a `create` function that takes as input data set `D` and the name of a supervised learning algorithm `algorithm_type` and produces as an output a computer program `predictor`. In the second line, you are using `predictor` to predict the label (`y_new` value) for a flower whose features are given by `x_new`.\n",
    "\n",
    "### kNN Algorithm\n",
    "kNN is a popular supervised learning algorithm that allows us to create `predictor`. The idea of kNN is that the label of flower `x_new` depends on labels of flowers in its neighborhood. In particular, kNN finds the distance between `x_new` and every example `x` in data set `D`. Then, it looks at the label `y` of k examples which are the closest to `x_new`. The predicted label `y_new` is obtained as the most common label in the group of the k nearest neighbors.\n",
    "\n",
    "**Parameter choice**. We need to make a few decisions when running kNN. The most important is the choice of `k`. If `k = 1`, then we are looking only at the hearest neighbor and it might not be a good idea if we are dealing with noisy data. If `k` is very large, then we might be counting far neighbors that might have different properties. Other decisions include the choice of distance metric (Euclidean is the standard one) and the choice whether to weight closer neighbors more than the farther ones.\n",
    "\n",
    "**Accuracy**. When deciding which parameters to pick or which supervised learning algorithm to use (there are popular algorithms other than kNN), the question is how to measure which choice is better. The answer is to check if `predictor` provides accurate prediction. Given a data set `D`, a typical way to check accuracy is to randomly split `D` into two data sets, `D_train` and `D_test`. Then, `predictor` is created/trained using `D_train` data set and its accuracy is checked using `D_test`. In particular, we use `predictor` to predict label of every example from `D_test` and compare it with the true labels. The percentage of the correct guesses on `D_test` is reported as accuracy of `predictor`.\n",
    "\n",
    "## kNN Demo\n",
    "The following piece of code is taken from:\n",
    "http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py. Let us run it (it is guaranteed to run with Python 2.7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 25   # how many nearest neighbors are consulted\n",
    "\n",
    "X = iris.data[:, [0,1]]  # we only take the first two features. We could\n",
    "y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "clf = neighbors.KNeighborsClassifier(k, weights='uniform')\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.figure(figsize=(6,3))   # this makes both axis equal \n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = %i, weights = '%s')\" % (k, 'uniform'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting figure shows the predictions of kNN when $k=1$. If `x_new` is in the blue region, the prediction will be the blue class. From this picture, we can observe a small blue blobs inside the predominantly gray area. This is because the nearest neighbor in this area is the blue example. \n",
    "\n",
    "**Question 1**. Change value of k to 3 and observe if there is any difference. Discuss what you see and why.\n",
    "\n",
    "**Answer**: Changing value of k to 3 from 1, makes the decision surface smooth and the borders among the three classes are noticeable and bit more clear as compared to when k = 1. \n",
    "\n",
    "**Question 2**. Change k to an even higher value, let us say to 25. What do we see now? Discuss.\n",
    "\n",
    "**Answer**: Changing k to 25 shows the boundaries among different regions more distinctly and clearly. The red class region size is more bigger now as compared to when k = 3. The green and blue class still overlap to some extent.\n",
    "\n",
    "**Queston 3**. In the line that creates `clf` change weights='uniform' to weights='distance'. Check the documentation or google to understand what it means. Explain. Run the code and discuss if you see any difference.\n",
    "\n",
    "**Answer**: weight='uniform' means all points in each class are equally weighted whereas weight='distance' means closer neighbors have a greater influence than neighbors which are further away.\n",
    "Changing weight='distance' shows the decision areas are bit smoother now like few green points that were in blue region earlier are in green region now. The boundary between blue region and green region has become more even now.\n",
    "\n",
    "**Question 4**. Take a look at the code and try to understand what each line of the code does. Explain each line of code.\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "k = 25 - k value is set i.e. the number of nearest neighbors to be consulted.\n",
    "\n",
    "X = iris.data[:, [0,1]] - X contains the first two features i.e. the columns sepal length and sepal width.\n",
    "\n",
    "y = iris.target - Label column 'y' contains the classes for the features.\n",
    "\n",
    "h = .02 spacing distance between values.\n",
    "\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF']) - colormap used in plot(for decision surface).\n",
    "\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF']) - color map used in plot(for training points).\n",
    "\n",
    "clf - An instance of Neighbours Classifier is created.\n",
    "\n",
    "clf.fit(X, y) - Fit the data X and y to the classifier.\n",
    "\n",
    "x_min, x_max - Define x_min, x_max as boundaries of x axis.\n",
    "\n",
    "y_min, y_max - Define y_min, y_max as boundaries of y axis.\n",
    "\n",
    "xx, yy - Meshgrid is created taking the min and max values of X and y.\n",
    "\n",
    "Z = clf.predict - Predicted values for all mesh points.\n",
    "\n",
    "Z = Z.reshape(xx.shape) - Reshape the matrix to correct shape.\n",
    "\n",
    "plt.figure() - To create a figure object.\n",
    "\n",
    "plt.figure(figsize=(6, 3)) - Figure with size 6 unit along width and 3 unit along height.\n",
    "\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light) - Show result into a color plot using light color scheme.\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold) - Show the training points using scatter with bold color scheme.\n",
    "\n",
    "plt.xlim(xx.min(), xx.max()) - set x limits.\n",
    "\n",
    "plt.ylim(yy.min(), yy.max()) - set y limits.\n",
    "\n",
    "plt.title - Specify a title of the plot.\n",
    "\n",
    "plt.show() - Show the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and testing\n",
    "  As mentioned above, the typical mechanism for testing accuracy of a `predictor` is to split the data randomly into training and testing, train `predictor` on training data and test its performance on test data. Let us see how it can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**. What is the size of the resulting objects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Size: (100, 2) \n",
      "X_test Size: (50, 2) \n",
      "y_train Size: (100,) \n",
      "y_test Size: (50,) \n"
     ]
    }
   ],
   "source": [
    "print(\"X_train Size: {} \".format(X_train.shape))\n",
    "print(\"X_test Size: {} \".format(X_test.shape))\n",
    "print(\"y_train Size: {} \".format(y_train.shape))\n",
    "print(\"y_test Size: {} \".format(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we created training and test sets, we can train a kNN classifier using the training data. Before moving forward, let us take a second and take a look at the documentation for kNN implementation in sklearn: http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html.\n",
    "\n",
    "Let us train the kNN predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n",
      "                     weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "k = 1   # number of nearest neighbors\n",
    "predictor = neighbors.KNeighborsClassifier(n_neighbors = k)\n",
    "predictor.fit(X_train, y_train);\n",
    "print (predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we trained `predictor` we can use it to provide predictions on any example `x`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.6 2.7]\n",
      " [6.  3. ]\n",
      " [6.4 3.1]\n",
      " [4.8 3.4]]\n",
      "[(2, 1), (2, 2), (2, 2), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "# select the first 4 test examples\n",
    "i = [0,1,2,3]\n",
    "x = X_test[i,:]\n",
    "print (x)\n",
    "# predict its label\n",
    "yhat = predictor.predict(x)\n",
    "# compare predicted and true labels\n",
    "print (list(zip(yhat,y_test[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**. Did your kNN predictor do a good job in predicting labels of the first 4 test examples? \n",
    "\n",
    "**Answer**: 3 test examples out of 4 are predicted correctly. Accuracy rate is 75%.\n",
    "\n",
    "**Question 7**. Write a piece of code that calculates the accuracy on those 4 test examples (number of correct guesses divided by the total number of guesses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracyFunc(y_predicted, y_actual):\n",
    "    a = len(y_actual[(y_predicted==y_actual)])/len(y_actual)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: {}\".format(accuracyFunc(yhat, y_test[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**. Find the predictions on all test examples in `X_test` and calculate the accuracy using your code from *Question 7*.\n",
    "\n",
    "Pay attention that methods in sklearn.neighbors.KNeighborsClassifier allow you to test the accuracy in a faster way (you should not use it to answer Questions 7 and 8):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.6 2.7]\n",
      " [6.  3. ]\n",
      " [6.4 3.1]\n",
      " [4.8 3.4]\n",
      " [5.6 3. ]\n",
      " [5.2 4.1]\n",
      " [7.  3.2]\n",
      " [5.  3. ]\n",
      " [5.4 3.7]\n",
      " [6.6 2.9]\n",
      " [6.  2.9]\n",
      " [5.1 3.5]\n",
      " [4.4 3.2]\n",
      " [5.6 2.9]\n",
      " [6.3 3.3]\n",
      " [5.8 2.7]\n",
      " [5.4 3.9]\n",
      " [6.7 3. ]\n",
      " [5.  3.5]\n",
      " [6.8 2.8]\n",
      " [5.7 2.6]\n",
      " [6.2 2.9]\n",
      " [5.1 3.8]\n",
      " [6.3 2.8]\n",
      " [6.8 3.2]\n",
      " [5.2 3.5]\n",
      " [6.7 3.1]\n",
      " [5.8 2.7]\n",
      " [5.9 3.2]\n",
      " [5.5 2.6]\n",
      " [4.3 3. ]\n",
      " [6.  2.2]\n",
      " [4.9 3.1]\n",
      " [5.1 3.4]\n",
      " [6.  2.7]\n",
      " [5.5 2.4]\n",
      " [4.7 3.2]\n",
      " [5.4 3.4]\n",
      " [6.2 2.8]\n",
      " [6.1 2.6]\n",
      " [6.5 3. ]\n",
      " [5.  3.6]\n",
      " [7.2 3.2]\n",
      " [7.4 2.8]\n",
      " [5.8 2.7]\n",
      " [7.7 3. ]\n",
      " [6.3 2.5]\n",
      " [6.5 3.2]\n",
      " [6.9 3.1]\n",
      " [6.7 3.3]]\n",
      "[(2, 1), (2, 2), (2, 2), (0, 0), (1, 1), (0, 0), (2, 1), (0, 0), (0, 0), (1, 1), (1, 1), (0, 0), (0, 0), (1, 1), (1, 2), (2, 1), (0, 0), (1, 2), (0, 0), (2, 1), (1, 1), (2, 1), (0, 0), (2, 2), (2, 2), (0, 0), (2, 1), (2, 1), (2, 1), (1, 1), (0, 0), (1, 2), (0, 0), (0, 0), (1, 1), (1, 1), (0, 0), (0, 0), (1, 2), (1, 2), (2, 2), (0, 0), (2, 2), (2, 2), (2, 2), (2, 2), (2, 1), (2, 2), (2, 2), (2, 2)]\n",
      "Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "x = X_test\n",
    "print (x)\n",
    "# predict its label\n",
    "yhat = predictor.predict(x)\n",
    "print (list(zip(yhat,y_test)))\n",
    "print(\"Accuracy: {}\".format(accuracyFunc(yhat, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "accuracy = predictor.score(X_test,y_test)\n",
    "print (accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9**. Train `predictor` using different choices of k. Try $k = 1, 3, 5, 15, 25, 50$. Report the accuracies on the test data (you can use the score method). Which choice of $k$ resulted in the highest accuracy? Comment briefly if the results make sense to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94\n",
      "0.94\n",
      "0.96\n",
      "0.96\n",
      "0.98\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "k_choices = [1, 3, 5, 15, 25, 50]\n",
    "for k in k_choices:\n",
    "    predictor = neighbors.KNeighborsClassifier(n_neighbors = k)\n",
    "    predictor.fit(X_train, y_train);\n",
    "    accuracy = predictor.score(X_test,y_test)\n",
    "    print (accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The highest accuracy of the model is when k = 25. The lowest accuracy is when k = 50. The accuracy differs every time we execute the model because the test data split/selection is different in each execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10**. Other than choice of $k$, `KNeighborsClassifier` allows you to make some other choices. For example, in *Question 3* you saw that you can use a weighted prediction. There are few other options. Study the documentation and summarize in few sentences what other options you have when training the kNN classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "Other than k, the options we can use when training the kNN classifier are -\n",
    "\n",
    "**1**. algorithm - This can be 'auto', 'ball_tree, 'kd_tree' or 'brute'. ‘auto’ type will attempt to decide the most appropriate algorithm based on the values passed to the fit method.\n",
    "\n",
    "**2**. leaf_size - This is passed for BallTree or KDTree type algorithm. This affects the speed of the construction, query and memory required to store the tree.\n",
    "\n",
    "**3**. metric - This is used to calculate the distance between the data points. The default metric is minkowski and with p = 2 which is equivalent to the standard Euclidean metric.\n",
    "\n",
    "**4**. p - This is the power parameter for Minkowski metric. When p = 1 this uses manhattan distance and when p = 2 it uses euclidean distance.\n",
    "\n",
    "**5**. weights - This has 2 paramaters 'uniform' and 'distance'. uniform' means all points in each class are equally weighted and 'distance' means closer neighbors have a greater influence than neighbors which are further away.               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11**. Train kNN classifier on a different pair of features of your choice. Use $k$ of your choice and feel free to keep other choices at their default values. Which pair of features results in higher accuracy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair of features:<br>\n",
    "Sepal Length, Sepal Width (Feature 0 and Feature 1)<br>\n",
    "Sepal Length, Petal Length (Feature 0 and Feature 2)<br>\n",
    "Sepal Length, Petal Width (Feature 0 and Feature 3)<br>\n",
    "Sepal Width, Petal Length (Feature 1 and Feature 2)<br>\n",
    "Sepal Width, Petal Width (Feature 1 and Feature 3)<br>\n",
    "Petal Length, Petal Width (Feature 2 and Feature 3)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature[0, 1], Accuracy is: 0.72\n",
      "Feature[0, 2], Accuracy is: 0.9\n",
      "Feature[0, 3], Accuracy is: 0.84\n",
      "Feature[1, 2], Accuracy is: 0.88\n",
      "Feature[1, 3], Accuracy is: 0.98\n",
      "Feature[2, 3], Accuracy is: 0.92\n"
     ]
    }
   ],
   "source": [
    "k = 25   # how many nearest neighbors are consulted\n",
    "X = iris.data \n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "features = [[0,1],[0,2],[0,3],[1,2],[1,3],[2,3]]\n",
    "\n",
    "for f in features:\n",
    "    predictor = neighbors.KNeighborsClassifier(k, weights='uniform')\n",
    "    predictor.fit(X_train[:, f], y_train)\n",
    "    accuracy = predictor.score(X_test[:, f],y_test)\n",
    "    print(\"Feature{}, Accuracy is: {}\".format(f,accuracy))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: The pair of features that results in higher accuracy is [1,3] with accuracy of 98%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 12**. Train kNN classifier using all 4 features. Report the accuracy on test data set. Play with parameters of kNN to try to find a combination that results in the highest accuracy. Can you find something that works better than $k=3$ and default choices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With k = 3 and default choices, Accuracy is: 0.96\n"
     ]
    }
   ],
   "source": [
    "k = 3   # how many nearest neighbors are consulted\n",
    "X = iris.data \n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "predictor = neighbors.KNeighborsClassifier(k, weights='uniform')\n",
    "predictor.fit(X_train, y_train)\n",
    "accuracy = predictor.score(X_test,y_test)\n",
    "print(\"With k = 3 and default choices, Accuracy is: {}\".format(accuracy))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With k = 25 and weights = distance, Accuracy is: 0.98\n"
     ]
    }
   ],
   "source": [
    "k = 25   # how many nearest neighbors are consulted\n",
    "X = iris.data \n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "predictor = neighbors.KNeighborsClassifier(k, weights='distance')\n",
    "predictor.fit(X_train, y_train)\n",
    "accuracy = predictor.score(X_test,y_test)\n",
    "print(\"With k = 25 and weights = distance, Accuracy is: {}\".format(accuracy))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Write your own function for kNN (50% of the score)\n",
    "\n",
    "**Question 13** Write a function for k-nearest neighbor (k-NN) classification of the form `accuracy = knnC(X_test, y_test, X_train, y_train, k)`, where *k* is the number of nearest neighbors. Assume the Euclidean distance. So, the inputs are training and test data and the output should be accuracy on test data. Repeat **Question 9** using your function. Check if the results are the same.\n",
    "**NOTE:** I know that you can find python code for this on Web. However, I specifically ask you to write this function on your own and not use anybody's help. This is one of the rare ML algorithms that can be quickly implemented. I think you will learn a lot from this experience that will be very useful for the remainder of this course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def euclideanDistance(testdata, traindata):\n",
    "    distance = 0\n",
    "    for i in range(len(testdata) - 1):\n",
    "        distance += ((testdata[i] - traindata[i])**2)\n",
    "    return math.sqrt(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import operator\n",
    "\n",
    "def knnC(X_test, y_test, X_train, y_train, k):\n",
    "    yhat = []\n",
    "    for i in range(len(X_test)): \n",
    "        neighbours = []\n",
    "        distance = []\n",
    "        for j in range(len(X_train)):\n",
    "            dist = euclideanDistance(X_test[i], X_train[j])\n",
    "            distance.append(((X_train[j], y_train[j]), dist))\n",
    "            \n",
    "        distance.sort(key=operator.itemgetter(1))\n",
    "        for x in range(k):\n",
    "            neighbours.append(distance[x][0])\n",
    "            \n",
    "        classVotes = [0, 0, 0]\n",
    "        for i in range(len(neighbours)):\n",
    "            response = neighbours[i][-1]\n",
    "            classVotes[response] += 1\n",
    "        index, value = max(enumerate(classVotes), key=operator.itemgetter(1))\n",
    "        yhat.append(index)\n",
    "    return (accuracyFunc(yhat, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94\n",
      "0.96\n",
      "0.96\n",
      "0.94\n",
      "0.92\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "k_choices = [1, 3, 5, 15, 25, 50]\n",
    "for k in k_choices:\n",
    "    accuracy = knnC(X_test, y_test, X_train, y_train, k)\n",
    "    print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
